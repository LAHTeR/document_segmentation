{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"MIG-08137aa2-e69b-5e74-8390-7997329b1336\"\n",
    "# os.environ[\"WORLD_SIZE\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and convert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing documents:   0%|          | 0/5 [00:00<?, ?doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row with inventory number 1171 due to status message: 'Niet gedigitaliseerd.'\n",
      "Skipping row with inventory number 2770 due to status message: 'Niet gedigitaliseerd.'\n",
      "Skipping row with inventory number 2770 due to status message: 'Niet gedigitaliseerd.'\n",
      "Skipping row with inventory number 2770 due to status message: 'Niet gedigitaliseerd.'\n",
      "Skipping row with inventory number 2911 due to status message: 'Niet gedigitaliseerd.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from document_segmentation.pagexml.generale_missiven import GeneraleMissiven\n",
    "from document_segmentation.settings import (\n",
    "    GENERALE_MISSIVEN_DOCUMENT_DIR,\n",
    "    GENERALE_MISSIVEN_SHEET,\n",
    ")\n",
    "\n",
    "N = None\n",
    "\n",
    "GENERALE_MISSIVEN_DOCUMENT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sheet = GeneraleMissiven(GENERALE_MISSIVEN_SHEET)\n",
    "\n",
    "existing_docs = {\n",
    "    path.stem\n",
    "    for path in GENERALE_MISSIVEN_DOCUMENT_DIR.glob(\"*.json\")\n",
    "    if path.is_file()\n",
    "}\n",
    "\n",
    "for document in tqdm(\n",
    "    sheet.to_documents(n=N, skip_ids=existing_docs),\n",
    "    total=(N or len(sheet)) - len(existing_docs),\n",
    "    desc=\"Writing documents\",\n",
    "    unit=\"doc\",\n",
    "):\n",
    "    document_file = GENERALE_MISSIVEN_DOCUMENT_DIR / f\"{document.id}.json\"\n",
    "\n",
    "    with document_file.open(\"xt\") as f:\n",
    "        f.write(document.model_dump_json())\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSON files: 100%|██████████| 100/100 [00:07<00:00, 13.43file/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20750"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from document_segmentation.model.dataset import PageDataset\n",
    "from document_segmentation.settings import MIN_REGION_TEXT_LENGTH\n",
    "\n",
    "dataset = PageDataset.from_dir(\n",
    "    GENERALE_MISSIVEN_DOCUMENT_DIR, n=100\n",
    ").remove_short_regions(MIN_REGION_TEXT_LENGTH)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({<Label.IN: 2>: 20552, <Label.BEGIN: 1>: 100, <Label.END: 3>: 98})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset._class_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[205.44554455445544, 1.0095849754293778, 209.59595959595958, 20750.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.class_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Page(label=<Label.IN: 2>, regions=[Region(id='region_9df849b1-56aa-48d5-b056-961b95080a78_1', types=(<RegionType.PARAGRAPH: 'paragraph'>, <RegionType.PHYSICAL_STRUCTURE_DOC: 'physical_structure_doc'>, <RegionType.TEXT_REGION: 'text_region'>, <RegionType.PAGEXML_DOC: 'pagexml_doc'>), coordinates=((927, 671), (946, 693), (949, 715), (1000, 725), (1044, 747), (1060, 766), (1047, 835), (990, 867), (930, 921), (937, 1054), (930, 1180), (943, 1221), (889, 1395), (892, 1417), (914, 1458), (895, 1528), (908, 1658), (902, 1750), (905, 1892), (883, 1974), (886, 2015), (876, 2053), (883, 2085), (867, 2155), (823, 2237), (788, 2265), (791, 2306), (807, 2329), (807, 2360), (845, 2385), (851, 2408), (848, 2461), (927, 2544), (921, 2563), (902, 2572), (873, 2604), (851, 2673), (854, 2705), (873, 2733), (876, 2851), (899, 2961), (902, 3123), (918, 3167), (918, 3192), (864, 3293), (870, 3331), (911, 3357), (952, 3366), (1206, 3366), (1338, 3379), (1598, 3376), (1772, 3385), (1971, 3376), (2091, 3392), (2256, 3366), (2307, 3366), (2411, 3398), (2449, 3436), (2474, 3448), (2509, 3452), (2538, 3363), (2538, 3290), (2528, 3249), (2503, 3202), (2509, 3164), (2509, 3066), (2519, 3018), (2506, 2857), (2519, 2828), (2509, 2648), (2522, 2613), (2515, 2518), (2541, 2373), (2528, 2310), (2541, 2287), (2544, 2192), (2512, 2110), (2512, 2085), (2528, 2060), (2522, 1901), (2512, 1867), (2525, 1769), (2512, 1689), (2515, 1541), (2525, 1506), (2525, 1395), (2496, 1243), (2468, 1158), (2487, 1025), (2462, 921), (2462, 889), (2481, 867), (2493, 816), (2490, 769), (2506, 718), (2503, 699), (2477, 664), (2449, 645), (2338, 655), (2291, 652), (2269, 658), (2114, 658), (2060, 649), (1807, 645), (1772, 652), (1408, 652), (1291, 645), (1206, 680), (1130, 677), (1031, 639)), lines=('Mallabaar en Godorine van beette die met zynen aanhang', 'en binnenlandde banden, al genogstaan daar van', 'meeter was Gevoorden, te', 'verdryven, dog thans de', 'verandert, en', 'Sammorgase alpeeten van wesen', 'Zin sedende en Malitie tot den', 'nog door', 'orlog tegens hem Gesoreert en sedwongen sijnde', 'Zal het ook daar om', \"'t Ecomp:s but en tijd tegeno\", 'Moeten weten, om hem door dien', '„woordig', 'zelven, oordog bij netorsie de forteeren, tot het', 'Eene ten voordeele van uEd:le ho: agtb: inde', \"boven gem: passagie van Cettorva, P' oordeel\", 'en bevonden sal werden te moeten Ledpen, derworgen', 'ook vermaand is, by onsen Eneraalen briev van'))], scan_nr=545, doc_id='NL-HaNA_1.04.02_1766_0545.jpg')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({<Label.IN: 2>: 16456, <Label.BEGIN: 1>: 73, <Label.END: 3>: 71})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = int(len(dataset) * TRAINING_DATA)\n",
    "\n",
    "training_data = dataset[:split]\n",
    "training_data._class_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({<Label.IN: 2>: 4096, <Label.END: 3>: 27, <Label.BEGIN: 1>: 27})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = dataset[split:]\n",
    "test_data._class_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "WEIGHTS = torch.Tensor(dataset.class_weights())  # For an imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from document_segmentation.model.page_sequence_tagger import PageSequenceTagger\n",
    "\n",
    "tagger = PageSequenceTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger._device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PageSequenceTagger(\n",
       "  (_page_embedding): PageEmbedding(\n",
       "    (_region_model): RegionEmbedding(\n",
       "      (_transformer_model): RobertaModel(\n",
       "        (embeddings): RobertaEmbeddings(\n",
       "          (word_embeddings): Embedding(42774, 768, padding_idx=1)\n",
       "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "          (token_type_embeddings): Embedding(1, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): RobertaEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (pooler): RobertaPooler(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "      )\n",
       "      (_region_type): Embedding(9, 16)\n",
       "      (_linear): Linear(in_features=784, out_features=4, bias=True)\n",
       "    )\n",
       "    (_cnn): Conv2d(100, 1, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (_linear): Linear(in_features=2, out_features=64, bias=True)\n",
       "  )\n",
       "  (_rnn): LSTM(64, 32, batch_first=True, dropout=0.1)\n",
       "  (_linear): Linear(in_features=32, out_features=4, bias=True)\n",
       "  (_softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1037.5 [00:01<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [1, 100, 3, 3], expected input[1, 16, 4, 4] to have 100 channels, but got 16 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWEIGHTS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LAHTeR/workspace/document-segmentation/document_segmentation/model/page_sequence_tagger.py:91\u001b[0m, in \u001b[0;36mPageSequenceTagger.train_\u001b[0;34m(self, pages, epochs, batch_size, weights)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m     88\u001b[0m     pages\u001b[38;5;241m.\u001b[39mbatches(batch_size), unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(pages) \u001b[38;5;241m/\u001b[39m batch_size\n\u001b[1;32m     89\u001b[0m ):\n\u001b[1;32m     90\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 91\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device)\n\u001b[1;32m     92\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch\u001b[38;5;241m.\u001b[39mlabel_tensor()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device))\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device\n\u001b[1;32m     94\u001b[0m     )\n\u001b[1;32m     96\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/LAHTeR/workspace/document-segmentation/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LAHTeR/workspace/document-segmentation/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/LAHTeR/workspace/document-segmentation/document_segmentation/model/page_sequence_tagger.py:52\u001b[0m, in \u001b[0;36mPageSequenceTagger.forward\u001b[0;34m(self, pages)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pages: PageDataset):\n\u001b[0;32m---> 52\u001b[0m     page_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_page_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m page_embeddings\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m (\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28mlen\u001b[39m(pages),\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_page_embedding\u001b[38;5;241m.\u001b[39moutput_size,\n\u001b[1;32m     57\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBad shape: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mpages.size()}\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m     rnn_out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rnn(page_embeddings)\n",
      "File \u001b[0;32m~/LAHTeR/workspace/document-segmentation/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LAHTeR/workspace/document-segmentation/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/LAHTeR/workspace/document-segmentation/document_segmentation/model/page_embedding.py:86\u001b[0m, in \u001b[0;36mPageEmbedding.forward\u001b[0;34m(self, pages)\u001b[0m\n\u001b[1;32m     79\u001b[0m region_inputs \u001b[38;5;241m=\u001b[39m pad_sequence(\n\u001b[1;32m     80\u001b[0m     [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_region_model(regions) \u001b[38;5;28;01mfor\u001b[39;00m regions \u001b[38;5;129;01min\u001b[39;00m regions_batch],\n\u001b[1;32m     81\u001b[0m     batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     82\u001b[0m     padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     83\u001b[0m )\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# rnn_out, hidden = self._rnn(region_inputs)\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m cnn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregion_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_linear(cnn_out)\n\u001b[1;32m     90\u001b[0m final_step_output_batch \u001b[38;5;241m=\u001b[39m out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/LAHTeR/workspace/document-segmentation/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LAHTeR/workspace/document-segmentation/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/LAHTeR/workspace/document-segmentation/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LAHTeR/workspace/document-segmentation/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [1, 100, 3, 3], expected input[1, 16, 4, 4] to have 100 channels, but got 16 channels instead"
     ]
    }
   ],
   "source": [
    "tagger.train_(training_data, EPOCHS, BATCH_SIZE, WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted\tActual\tPage ID\tText\tScores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/129.6875 [00:00<00:05, 22.72batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0317.jpg\tzouden doen dragen, dat geene dubbelde; dukatons z\t[0.9711317420005798, 0.028105031698942184, 0.00037071146653033793, 0.00039246713276952505]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0318.jpg\taanbesteede; 'sComp„s reparatie tot Th: 100:; mede\t[0.6227352619171143, 0.37679725885391235, 0.00024215613666456193, 0.00022533860465046018]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0349.jpg\tworden gelijd, dat die frifatie/ bedongen inkoop; \t[0.9723415374755859, 0.026956947520375252, 0.00034019837039522827, 0.00036138645373284817]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0350.jpg\tbegunstigde Britsche vrienden van ons; afkeerig is\t[0.598449170589447, 0.40105879306793213, 0.00025473529240116477, 0.00023724768834654242]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0381.jpg\t„meld onder aanhaaling, dat de noodsaake„; lijkhei\t[0.9586215019226074, 0.04012445732951164, 0.0006167643587104976, 0.0006373219075612724]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0382.jpg\tdat de Comt. e en de Colonie ginter volgens der; m\t[0.6955246925354004, 0.3040960729122162, 0.00019765015167649835, 0.0001815684954635799]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0413.jpg\teenige verandering gemaakt, en deselve; insteede v\t[0.9544506072998047, 0.04402528330683708, 0.0007477274048142135, 0.000776338332798332]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0414.jpg\tleggen van hout ter hoogte van 16 a 17 dui„; men o\t[0.5804937481880188, 0.41866335272789, 0.00043919976451434195, 0.00040371480281464756]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0445.jpg\thij in maij 1710 tot ƒ 43621. 14. — deber stond,; \t[0.9531462788581848, 0.04521990567445755, 0.0008021294488571584, 0.0008315807208418846]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0446.jpg\tBaniedsja als een zoon van den geweesene kas„; sem\t[0.5838788747787476, 0.4151783585548401, 0.0004915518802590668, 0.00045124764437787235]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0477.jpg\thebben; op uwel Edele Hoog Agtb. ter; somme van ƒ1\t[0.9556076526641846, 0.042926110327243805, 0.0007193837664090097, 0.0007468336261808872]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0478.jpg\thij nu verpligt was aanteneemen; voor de Compagnie\t[0.5682927370071411, 0.4307789206504822, 0.0004841084883082658, 0.0004442463396117091]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 9/129.6875 [00:00<00:05, 23.66batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0509.jpg\tDat particuliere Engelsen alleen met; sal peter na\t[0.9645834565162659, 0.034492310136556625, 0.00045255740405991673, 0.00047164104762487113]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0510.jpg\t7dd; „o of ƒ 13. 17 2/5. , dan bleek het schie„; l\t[0.556537389755249, 0.44290244579315186, 0.0002930821792688221, 0.0002670731919351965]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0541.jpg\tvan'teen en ander word verder gehandeld in; de bri\t[0.9697939157485962, 0.029392404481768608, 0.00039518537232652307, 0.0004184711433481425]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0542.jpg\tmissive van den 8:' october 1777. § 300:/ En; ton \t[0.5456979274749756, 0.453619122505188, 0.00035544708953239024, 0.000327510351780802]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0573.jpg\tEen somma van 800 ropijen sijnde ƒ 1000. boven; de\t[0.9778214693069458, 0.02146643027663231, 0.0003412238438613713, 0.00037098146276548505]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0574.jpg\telkander te krijgen was, dienen moest voor; het fo\t[0.5787616968154907, 0.4202612340450287, 0.0005027833394706249, 0.0004742993914987892]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0605.jpg\tIn slot deses hebben wij de minister nog; een Exra\t[0.972501814365387, 0.026775570586323738, 0.00035078986547887325, 0.0003718380175996572]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0606.jpg\tals in so verre dat wij in onse Jongste besoigne; \t[0.6407743096351624, 0.3587900698184967, 0.00022536868345923722, 0.00021025991009082645]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 12/129.6875 [00:00<00:04, 24.83batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0637.jpg\twas gereverteerd.; uit kunnen aan ons gerigten bri\t[0.9554650783538818, 0.043025724589824677, 0.0007408924866467714, 0.0007683063158765435]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0638.jpg\t30:e aug:s naar Jaggernaike poerom hadden; gesonde\t[0.5908240079879761, 0.4083344638347626, 0.00043875601841136813, 0.00040269046439789236]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0669.jpg\toorlog tussen Engeland en V rankeryk ont„; stond, \t[0.9506711363792419, 0.04754258319735527, 0.0008773644804023206, 0.0009088683873414993]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0670.jpg\tbragt dat een bhaarkoper maar op 88 à 91½; N. N: P\t[0.6131911277770996, 0.38599371910095215, 0.0004243984294589609, 0.0003907752688974142]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0701.jpg\tsij vermeenen dierhalven ook dat't; 578; weder in \t[0.9417372345924377, 0.055552516132593155, 0.0013279347913339734, 0.0013822790933772922]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0702.jpg\tvopgen, tegens Een halv procento interess; smaands\t[0.643269419670105, 0.3552343249320984, 0.0007725002360530198, 0.0007236621459014714]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0733.jpg\taansien en authoribeit in't algemeen, en de; eers \t[0.9696403741836548, 0.029544444754719734, 0.00039662481867708266, 0.00041857166797854006]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0734.jpg\tteffens, eenige daan bij voorkomende min„; bruiken\t[0.6091956496238708, 0.39029866456985474, 0.00026228511705994606, 0.00024340924574062228]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 15/129.6875 [00:00<00:05, 21.19batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0765.jpg\tIndien het schip than van; Bengale op Ceilon aangi\t[0.9911278486251831, 0.008677499368786812, 9.08474758034572e-05, 0.00010382452455814928]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0766.jpg\top dat het zoude sijn geweest die Caneel vroeg:; t\t[0.7399224638938904, 0.2598026990890503, 0.00013959332136437297, 0.00013519941421691328]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0797.jpg\t12:e november aan ons order versogt hebbende; of z\t[0.976003110408783, 0.02348206751048565, 0.00024965047487057745, 0.00026518694357946515]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0798.jpg\taan den Politicquen Raad sal moeten addres„; seere\t[0.6484968662261963, 0.3511802554130554, 0.0001672081125434488, 0.00015563216584268957]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0829.jpg\teen honderd negen en twintig snaphanen,; alle volg\t[0.9520689249038696, 0.04627956822514534, 0.0008104535518214107, 0.0008409757283516228]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0830.jpg\tovergeleverde, en vrij overhaastig beeedigde; rapp\t[0.5782731175422668, 0.4208219051361084, 0.0004712384252343327, 0.0004337275167927146]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 21/129.6875 [00:01<00:05, 19.91batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0861.jpg\t§: 24 Tot secretaris van Politie insteede van den \t[0.970413327217102, 0.028797650709748268, 0.0003831917711067945, 0.00040593123412691057]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0862.jpg\t§ 37 Ten besluite deesor materie brengen; wij uwe \t[0.5353243947029114, 0.46393710374832153, 0.00038453287561424077, 0.00035390572156757116]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0893.jpg\tsal himn geweest voor een enkelde keer, Conde„; mi\t[0.9859957098960876, 0.013638210482895374, 0.00017319867038168013, 0.00019281634013168514]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0894.jpg\tgewoonte verschonken, ter somma van; rop„s 1100 of\t[0.5715686082839966, 0.4277229905128479, 0.0003650063299573958, 0.00034344871528446674]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0925.jpg\t§ 14 Daar en tegen beloopt het versondene. der„; w\t[0.9782046675682068, 0.02134193666279316, 0.00021919111895840615, 0.0002341189974686131]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0926.jpg\t§ 16. van de goede uitwvecking, die in het nouwe; \t[0.576147198677063, 0.4234517514705658, 0.00020895927445963025, 0.00019214280473534018]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0957.jpg\t§ 51 Inselvdervoegen, permitteerden wij bij briev;\t[0.9712193608283997, 0.02803003415465355, 0.00036432998604141176, 0.0003862180165015161]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0958.jpg\tsolitie te vooren, het honorabel ont„; slag van de\t[0.6188614964485168, 0.3806733787059784, 0.0002406255662208423, 0.00022447353694587946]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0989.jpg\t„ kundige lieden, met een behoorlijke instrucke en\t[0.9773436784744263, 0.022177690640091896, 0.00023157281975727528, 0.0002470542094670236]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_0990.jpg\t§ 37 Dat omtrent ses; militairen die lang over hun\t[0.516672670841217, 0.4827907085418701, 0.00028042966732755303, 0.0002561404253356159]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 27/129.6875 [00:01<00:07, 14.39batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_1021.jpg\tBij het Collegie van; schepenen is, ter plaatsvull\t[0.9687738418579102, 0.030365020036697388, 0.00041935141780413687, 0.0004418338357936591]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_1022.jpg\t§7 In voldoening aan de resolutie van; den 4:e mai\t[0.5564950704574585, 0.4428122937679291, 0.0003609151463024318, 0.00033172869007103145]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_1053.jpg\twij de vrijheid uwe wel Edele Hoog Agtb.; in eerbi\t[0.9780625700950623, 0.021477222442626953, 0.00022278404503595084, 0.00023733246780466288]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_1054.jpg\tduurte van het gean er nog is, geen gevolg; kunnen\t[0.6789174675941467, 0.3207969069480896, 0.00014789852139074355, 0.00013770180521532893]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_1085.jpg\tIaccatrasche -Boven - en Treanger- landen; omtrend\t[0.9890824556350708, 0.010637151077389717, 0.00013170110469218343, 0.00014874101907480508]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_1086.jpg\tde rivier Tjimandirie, laaten hooft vatten,; door \t[0.5883798003196716, 0.41092678904533386, 0.00035614921944215894, 0.00033722969237715006]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_1117.jpg\tTot opperchirurgijn van het was en an„; meuluus, e\t[0.9683162569999695, 0.03077838011085987, 0.00044129067100584507, 0.0004640152910724282]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_3533_1118.jpg\t§ 143 Onder de; werkbaasen is eenlijk de Eerste we\t[0.8000038266181946, 0.19970418512821198, 0.00014777136675547808, 0.00014430240844376385]\n",
      "IN\tEND\tNL-HaNA_1.04.02_3533_1133.jpg\tuwer Wel Edele Hoog Agtb. ootmoedige en; seer geho\t[0.019810102880001068, 0.9801065921783447, 4.3816100514959544e-05, 3.943330739275552e-05]\n",
      "IN\tBEGIN\tNL-HaNA_1.04.02_1603_0016.jpg\tPatria; Aan d' Ed„le ho: agtb: Heeren bewindhebber\t[0.02380579151213169, 0.9761202931404114, 3.8792881241533905e-05, 3.511643808451481e-05]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_1603_0031.jpg\tZemwegens de boeginesen in het proti; sijn de bewi\t[0.9922412633895874, 0.007581387646496296, 8.246129436884075e-05, 9.489273361396044e-05]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_1603_0032.jpg\tdien Co:s daar over door onse ministers aange„; „s\t[0.7128308415412903, 0.28684911131858826, 0.00016327066987287253, 0.00015683210222050548]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 32/129.6875 [00:01<00:05, 17.39batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\tIN\tNL-HaNA_1.04.02_1603_0063.jpg\td' oude daatsen te probeeren, welke door ses; geco\t[0.9848355650901794, 0.014892235398292542, 0.0001302896998822689, 0.00014186282351147383]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_1603_0064.jpg\tontvangen, maar naderhand heevd sijn E. .; jaarige\t[0.6311376094818115, 0.36857086420059204, 0.000151675587403588, 0.0001398552703903988]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_1603_0095.jpg\td'oorlogs munitien en verdere gereedsz:; daar toe \t[0.9802466630935669, 0.019347678869962692, 0.00019583785615395755, 0.0002098510303767398]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_1603_0096.jpg\ts der drie beloovde predikanten; te wel te pas gek\t[0.8027003407478333, 0.1970936506986618, 0.000104518148873467, 0.00010153818584512919]\n",
      "IN\tEND\tNL-HaNA_1.04.02_1603_0119.jpg\tden advocaat fiscaal van India tot sijne; laste sa\t[0.023968417197465897, 0.9759633541107178, 3.5646702599478886e-05, 3.25499931932427e-05]\n",
      "IN\tBEGIN\tNL-HaNA_1.04.02_2768_0021.jpg\tAan d'Edele Hoog Agtb: Heeren, de Heeren; Represen\t[0.020081723108887672, 0.9798445701599121, 3.8536647480214015e-05, 3.5133958590449765e-05]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_2768_0028.jpg\tg'ordonneert; Dat men omtrent de Journalen so; wel\t[0.9546604156494141, 0.04385862499475479, 0.0007265973254106939, 0.0007542868261225522]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_2768_0029.jpg\ttot dat UwEdele Hoog agtb: ons na; „der ordre geli\t[0.5812397599220276, 0.41798821091651917, 0.000402131729060784, 0.0003698791842907667]\n",
      "IN\tEND\tNL-HaNA_1.04.02_2768_0058.jpg\tagting te blijven./; WEdele Hoog Agt„; „bare Onder\t[0.008880747482180595, 0.9909960627555847, 6.381871207850054e-05, 5.9443504142109305e-05]\n",
      "IN\tBEGIN\tNL-HaNA_1.04.02_1382_0781.jpg\tAan d' Ed=le heeren bewinthebberen; vande generale\t[0.006488349288702011, 0.9933217167854309, 9.658704220782965e-05, 9.335175127489492e-05]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_0782.jpg\tEngte in ruijm zee geraackt sijn, nadien; Het Iagh\t[0.9639056921005249, 0.03515264764428139, 0.00046112932614050806, 0.0004805362259503454]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_0783.jpg\tnegen uuren uijt dese werelt te roepen,; En tot si\t[0.5456782579421997, 0.453733891248703, 0.0003075232380069792, 0.0002804007090162486]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_0814.jpg\tuuren begonnen sijnde; heeft men aldaar; nogh den \t[0.9711479544639587, 0.028110260143876076, 0.00036014686338603497, 0.0003815729869529605]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_0815.jpg\tnae verdelinge op dese retourvloot van; 483250: lb\t[0.6191608309745789, 0.38038715720176697, 0.00023386499378830194, 0.000218102170038037]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 35/129.6875 [00:01<00:05, 17.40batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_0846.jpg\tMacasser. maar ƒ 25840: 14: 7: en hunne incomsten;\t[0.9416800141334534, 0.055683065205812454, 0.0012922778259962797, 0.0013446594821289182]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_0847.jpg\tMacasser. particulieren aanvoer te sluijten / sal \t[0.6336658596992493, 0.3649386167526245, 0.0007205404108390212, 0.0006750066531822085]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_0878.jpg\tPalembangh. en den pangeran depattij; En hebben; d\t[0.9454439878463745, 0.05201345309615135, 0.0012471993686631322, 0.0012953290715813637]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_0879.jpg\tPalembangh, hem souden hebben geweest, wederlijdse\t[0.8305928111076355, 0.16889292001724243, 0.00025930453557521105, 0.0002548944321461022]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_0910.jpg\tvan een duijsend Roa:, de minste; verstreckinge me\t[0.9530431628227234, 0.045372091233730316, 0.0007778365397825837, 0.0008069067262113094]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_0911.jpg\tonsen Iongste sullen vinden aangehaalt;; sijnde he\t[0.5811167359352112, 0.41804447770118713, 0.00043691243627108634, 0.0004018777690362185]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_0942.jpg\tBengale. soude weten te doen, behalven dat sij des\t[0.9409396648406982, 0.056308720260858536, 0.001348405028693378, 0.0014032431645318866]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_0943.jpg\tBengale gedreven, En nu en dan nogh al ijets; vand\t[0.6359952688217163, 0.3624129593372345, 0.0008219273877330124, 0.0007698566187173128]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 37/129.6875 [00:07<01:07,  1.38batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_0974.jpg\tCormandel, haar daarmede gesuspecteert heeft.; bel\t[0.9538847804069519, 0.044573552906513214, 0.0007564668194390833, 0.0007852856651879847]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_0975.jpg\tCormandel, gewoonte aldaar is, de goederen te late\t[0.5918629765510559, 0.40737321972846985, 0.0003975749423261732, 0.00036617935984395444]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_1006.jpg\tCeijlon. fruijten, uijt persia den 31: october op \t[0.968643069267273, 0.030483657494187355, 0.00042528484482318163, 0.00044794505811296403]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_1007.jpg\tals ons en alomme in India present; met de nodige \t[0.6354628801345825, 0.3640614449977875, 0.00024647967074997723, 0.00022918944887351245]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 39/129.6875 [00:13<01:48,  1.19s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_1038.jpg\tons maar wat Rijckelijcken daar van; mede deelen, \t[0.9557549953460693, 0.04284050688147545, 0.000688561296556145, 0.0007159532397054136]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_1039.jpg\tSouratta. volgens hun schrijven van 20: Iunij; voo\t[0.559892475605011, 0.43927687406539917, 0.0004326395282987505, 0.0003979949397034943]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 40/129.6875 [00:15<02:01,  1.36s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_1070.jpg\ten winsten te staan, om te meer; vertier te maacke\t[0.941635012626648, 0.055687323212623596, 0.0013125929981470108, 0.0013651762856170535]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_1071.jpg\tIavas oostcust, vermogen niet om 'tselve buijten o\t[0.6460748910903931, 0.35258641839027405, 0.0006912039243616164, 0.0006474517867900431]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 41/129.6875 [00:17<02:13,  1.50s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_1102.jpg\tBatavia. Ult=o maart 1683) nevens desen dubbel; to\t[0.9646387100219727, 0.03359111398458481, 0.000855210586450994, 0.0009149403194896877]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_1103.jpg\tBatavia. beright off onderright nogh bij geschrift\t[0.771961510181427, 0.2270055115222931, 0.0005194319528527558, 0.000513597798999399]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 42/129.6875 [00:21<03:02,  2.08s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_1134.jpg\tIongste voijagie nae sumatras weltcust,; omtrent h\t[0.9712966084480286, 0.02808092162013054, 0.00030409294413402677, 0.00031838411814533174]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_1135.jpg\tis, maar Evenwel bestaat uijt swacke; En sterffeli\t[0.587746798992157, 0.4118863046169281, 0.00019225322466809303, 0.00017464635311625898]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 43/129.6875 [00:25<03:26,  2.38s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_1166.jpg\tNoch P=r de scheepen thuijs te; spijck en strijen \t[0.9860454201698303, 0.013723908923566341, 0.00011015445488737896, 0.00012048761709593236]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_1382_1167.jpg\twijnen &=a met de schepen vande; Equipagie des Iaa\t[0.6057382822036743, 0.39396899938583374, 0.00015266861009877175, 0.00014006120909471065]\n",
      "IN\tEND\tNL-HaNA_1.04.02_1382_1189.jpg\tBatavia. sij, 'tgunt wij versoecken dat uw Ho: Ed=\t[0.024106426164507866, 0.9758249521255493, 3.59416808350943e-05, 3.2722273317631334e-05]\n",
      "IN\tBEGIN\tNL-HaNA_1.04.02_2140_0043.jpg\tOriginele; Generale Missive van Haar; Edelens Den \t[0.024853648617863655, 0.975078821182251, 3.52835631929338e-05, 3.214679964003153e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 44/129.6875 [00:27<03:22,  2.37s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\tIN\tNL-HaNA_1.04.02_2140_0051.jpg\tmissive door den Gouverneur, en; raad aan Cabo de \t[0.9406207203865051, 0.05659669637680054, 0.001364226802252233, 0.001418375875800848]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_2140_0052.jpg\ten den Crinas vaerder Evengemelte maand na neder„;\t[0.6673074960708618, 0.3315068483352661, 0.0006114543648436666, 0.0005741908098571002]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 45/129.6875 [00:29<03:17,  2.33s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\tIN\tNL-HaNA_1.04.02_2140_0083.jpg\tmen had den sulth: gelijk aangetoont, de schadelyk\t[0.93804931640625, 0.05894046276807785, 0.0014762328937649727, 0.0015340198297053576]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_2140_0084.jpg\tal Eenigen tijd te hebben huijs; gehouden op het E\t[0.6626120805740356, 0.3360038995742798, 0.0007136655622161925, 0.0006703117396682501]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 46/129.6875 [00:32<03:17,  2.36s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\tIN\tNL-HaNA_1.04.02_2140_0115.jpg\tin dese directie met de scheepen; oostendenaren am\t[0.940448522567749, 0.05680786445736885, 0.0013455083826556802, 0.0013981197262182832]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_2140_0116.jpg\tmontant van der„ aan vaderlandse - en Indische; re\t[0.6487979292869568, 0.34989917278289795, 0.0006724374834448099, 0.0006305210990831256]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 47/129.6875 [00:34<03:10,  2.31s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\tIN\tNL-HaNA_1.04.02_2140_0147.jpg\tal het geene de ministers op; den; Eijsch van uEd:\t[0.9437512755393982, 0.053762540221214294, 0.0012183841317892075, 0.0012677614577114582]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_2140_0148.jpg\tde scheepen; wickenburg en; is vorderde goederen h\t[0.6372575163841248, 0.36147570610046387, 0.0006543862400576472, 0.0006124170613475144]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 48/129.6875 [00:37<01:03,  1.29batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN\tIN\tNL-HaNA_1.04.02_2140_0179.jpg\tdie nergens toe kan werden; g' Emploijeert maar oo\t[0.9455181956291199, 0.052035294473171234, 0.0011997788678854704, 0.001246739993803203]\n",
      "BEGIN\tIN\tNL-HaNA_1.04.02_2140_0180.jpg\tovergaan Een monster, ter qu; titeijt van 37½ lb:,\t[0.8264032602310181, 0.1730942577123642, 0.0002529376943130046, 0.0002495023945812136]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 30\u001b[0m\n\u001b[1;32m     25\u001b[0m f1_score \u001b[38;5;241m=\u001b[39m MulticlassF1Score(average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(Label))\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m     28\u001b[0m     test_data\u001b[38;5;241m.\u001b[39mbatches(BATCH_SIZE), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_data) \u001b[38;5;241m/\u001b[39m BATCH_SIZE, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m ):\n\u001b[0;32m---> 30\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m \u001b[43mtagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mlabels()\n\u001b[1;32m     33\u001b[0m     _labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([label\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m~/LAHTeR/workspace/document-segmentation/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LAHTeR/workspace/document-segmentation/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/LAHTeR/workspace/document-segmentation/document_segmentation/model/page_sequence_tagger.py:52\u001b[0m, in \u001b[0;36mPageSequenceTagger.forward\u001b[0;34m(self, pages)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pages: PageDataset):\n\u001b[0;32m---> 52\u001b[0m     page_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_page_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m page_embeddings\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m (\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28mlen\u001b[39m(pages),\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_page_embedding\u001b[38;5;241m.\u001b[39moutput_size,\n\u001b[1;32m     57\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBad shape: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mpages.size()}\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m     rnn_out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rnn(page_embeddings)\n",
      "File \u001b[0;32m~/LAHTeR/workspace/document-segmentation/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LAHTeR/workspace/document-segmentation/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/LAHTeR/workspace/document-segmentation/document_segmentation/model/page_embedding.py:76\u001b[0m, in \u001b[0;36mPageEmbedding.forward\u001b[0;34m(self, pages)\u001b[0m\n\u001b[1;32m     67\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToo many regions (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(regions_batch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), truncating to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_regions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m     )\n\u001b[1;32m     70\u001b[0m     region_inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m         regions_batch[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_regions \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;241m+\u001b[39m regions_batch[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_regions \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m :]\n\u001b[1;32m     73\u001b[0m     )\n\u001b[1;32m     75\u001b[0m region_inputs \u001b[38;5;241m=\u001b[39m pad_sequence(\n\u001b[0;32m---> 76\u001b[0m     \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_region_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mregions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mregions_batch\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     77\u001b[0m     batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     78\u001b[0m     padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     79\u001b[0m )\n\u001b[1;32m     81\u001b[0m rnn_out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rnn(region_inputs)\n\u001b[1;32m     83\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_linear(rnn_out)\n",
      "File \u001b[0;32m~/LAHTeR/workspace/document-segmentation/document_segmentation/model/page_embedding.py:76\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     67\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToo many regions (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(regions_batch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), truncating to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_regions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m     )\n\u001b[1;32m     70\u001b[0m     region_inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m         regions_batch[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_regions \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;241m+\u001b[39m regions_batch[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_regions \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m :]\n\u001b[1;32m     73\u001b[0m     )\n\u001b[1;32m     75\u001b[0m region_inputs \u001b[38;5;241m=\u001b[39m pad_sequence(\n\u001b[0;32m---> 76\u001b[0m     [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_region_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregions\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m regions \u001b[38;5;129;01min\u001b[39;00m regions_batch],\n\u001b[1;32m     77\u001b[0m     batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     78\u001b[0m     padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     79\u001b[0m )\n\u001b[1;32m     81\u001b[0m rnn_out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rnn(region_inputs)\n\u001b[1;32m     83\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_linear(rnn_out)\n",
      "File \u001b[0;32m~/LAHTeR/workspace/document-segmentation/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LAHTeR/workspace/document-segmentation/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/LAHTeR/workspace/document-segmentation/document_segmentation/model/region_embedding.py:141\u001b[0m, in \u001b[0;36mRegionEmbedding.forward\u001b[0;34m(self, regions)\u001b[0m\n\u001b[1;32m    132\u001b[0m expected_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(regions), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_embedding_size)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    134\u001b[0m     text_embeddings\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m expected_size\n\u001b[1;32m    135\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape was \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_embeddings\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but should be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m region_types \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_region_type(\n\u001b[1;32m    139\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIntTensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mRegionType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mregion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtypes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mregion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mregions\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m--> 141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     )\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m regions\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_region_type\u001b[38;5;241m.\u001b[39membedding_dim)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device)\n\u001b[1;32m    145\u001b[0m )\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# region_coordinates = self._coordinates_tensor(region).float() TODO\u001b[39;00m\n\u001b[1;32m    149\u001b[0m region_inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m    150\u001b[0m     [\n\u001b[1;32m    151\u001b[0m         text_embeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m     dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    156\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import sys\n",
    "\n",
    "from torcheval.metrics import (\n",
    "    MulticlassAccuracy,\n",
    "    MulticlassF1Score,\n",
    "    MulticlassPrecision,\n",
    "    MulticlassRecall,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "from document_segmentation.pagexml.datamodel.label import Label\n",
    "\n",
    "writer = csv.DictWriter(\n",
    "    sys.stdout,\n",
    "    fieldnames=(\"Predicted\", \"Actual\", \"Page ID\", \"Text\", \"Scores\"),\n",
    "    delimiter=\"\\t\",\n",
    ")\n",
    "\n",
    "writer.writeheader()\n",
    "\n",
    "accuracy = MulticlassAccuracy(num_classes=len(Label))\n",
    "precision = MulticlassPrecision(average=None, num_classes=len(Label))\n",
    "recall = MulticlassRecall(average=None, num_classes=len(Label))\n",
    "f1_score = MulticlassF1Score(average=None, num_classes=len(Label))\n",
    "\n",
    "for batch in tqdm(\n",
    "    test_data.batches(BATCH_SIZE), total=len(test_data) / BATCH_SIZE, unit=\"batch\"\n",
    "):\n",
    "    predicted = tagger(batch)\n",
    "    labels = batch.labels()\n",
    "\n",
    "    _labels = torch.Tensor([label.value - 1 for label in labels]).to(int)\n",
    "    accuracy.update(predicted, _labels)\n",
    "    precision.update(predicted, _labels)\n",
    "    recall.update(predicted, _labels)\n",
    "    f1_score.update(predicted, _labels)\n",
    "\n",
    "    for page, pred, label in zip(batch.pages, predicted, labels):\n",
    "        pred_label = Label(pred.argmax().item() + 1)\n",
    "        if pred_label != Label.IN or label != Label.IN:\n",
    "            writer.writerow(\n",
    "                {\n",
    "                    \"Predicted\": pred_label.name,\n",
    "                    \"Actual\": label.name,\n",
    "                    \"Page ID\": page.doc_id,\n",
    "                    \"Text\": page.text(delimiter=\"; \")[:50],\n",
    "                    \"Scores\": str(pred.tolist()),\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:tensor([[2],\n",
      "        [3]]) classes have zero instances in both the predictions and the ground truth labels. Precision is still logged as zero.\n",
      "WARNING:root:One or more NaNs identified, as no ground-truth instances of [0, 2, 3] have been seen. These have been converted to zero.\n",
      "WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric\tAverage\tBEGIN\tIN\tEND\tOUT\n",
      "MulticlassPrecision\tNone\t0.0000\t1.0000\t0.0000\t0.0000\n",
      "MulticlassRecall\tNone\t0.0000\t0.9300\t0.0000\t0.0000\n",
      "MulticlassF1Score\tNone\t0.0000\t0.9637\t0.0000\t0.0000\n",
      "Accuracy (micro average):\t0.9300\n"
     ]
    }
   ],
   "source": [
    "writer = csv.DictWriter(\n",
    "    sys.stdout,\n",
    "    fieldnames=[\"Metric\", \"Average\"] + [label.name for label in Label],\n",
    "    delimiter=\"\\t\",\n",
    ")\n",
    "writer.writeheader()\n",
    "\n",
    "for metric in (precision, recall, f1_score):\n",
    "    scores = {\n",
    "        label.name: f\"{score:.4f}\"\n",
    "        for label, score in zip(Label, metric.compute().tolist())\n",
    "    }\n",
    "    writer.writerow(\n",
    "        {\"Metric\": metric.__class__.__name__, \"Average\": str(metric.average)} | scores\n",
    "    )\n",
    "\n",
    "print(f\"Accuracy ({accuracy.average} average):\\t{accuracy.compute().item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
