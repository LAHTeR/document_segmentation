{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"MIG-08137aa2-e69b-5e74-8390-7997329b1336\"\n",
    "# os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and convert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing documents:   0%|          | 0/5 [00:00<?, ?doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row with inventory number 1171 due to status message: 'Niet gedigitaliseerd.'\n",
      "Skipping row with inventory number 2770 due to status message: 'Niet gedigitaliseerd.'\n",
      "Skipping row with inventory number 2770 due to status message: 'Niet gedigitaliseerd.'\n",
      "Skipping row with inventory number 2770 due to status message: 'Niet gedigitaliseerd.'\n",
      "Skipping row with inventory number 2911 due to status message: 'Niet gedigitaliseerd.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from document_segmentation.pagexml.generale_missiven import GeneraleMissiven\n",
    "from document_segmentation.settings import (\n",
    "    GENERALE_MISSIVEN_DOCUMENT_DIR,\n",
    "    GENERALE_MISSIVEN_SHEET,\n",
    ")\n",
    "\n",
    "N = None\n",
    "\n",
    "GENERALE_MISSIVEN_DOCUMENT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sheet = GeneraleMissiven(GENERALE_MISSIVEN_SHEET)\n",
    "\n",
    "existing_docs = {\n",
    "    path.stem\n",
    "    for path in GENERALE_MISSIVEN_DOCUMENT_DIR.glob(\"*.json\")\n",
    "    if path.is_file()\n",
    "}\n",
    "\n",
    "for document in tqdm(\n",
    "    sheet.to_documents(n=N, skip_ids=existing_docs),\n",
    "    total=(N or len(sheet)) - len(existing_docs),\n",
    "    desc=\"Writing documents\",\n",
    "    unit=\"doc\",\n",
    "):\n",
    "    document_file = GENERALE_MISSIVEN_DOCUMENT_DIR / f\"{document.id}.json\"\n",
    "\n",
    "    with document_file.open(\"xt\") as f:\n",
    "        f.write(document.model_dump_json())\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSON files:  21%|██▏       | 194/909 [00:16<00:23, 30.71file/s]WARNING:root:No pages found in document id='352' inventory_nr=1684 inventory_part=None pages=[].\n",
      "Reading JSON files:  68%|██████▊   | 614/909 [01:00<00:19, 15.39file/s]WARNING:root:No pages found in document id='412' inventory_nr=1887 inventory_part=None pages=[].\n",
      "Reading JSON files:  68%|██████▊   | 620/909 [01:00<00:18, 15.34file/s]WARNING:root:No pages found in document id='353' inventory_nr=1684 inventory_part=None pages=[].\n",
      "Reading JSON files:  96%|█████████▌| 869/909 [01:24<00:04,  8.98file/s]WARNING:root:No pages found in document id='354' inventory_nr=1684 inventory_part=None pages=[].\n",
      "Reading JSON files: 100%|██████████| 909/909 [01:32<00:00,  9.79file/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "191146"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from document_segmentation.model.dataset import PageDataset\n",
    "\n",
    "dataset = PageDataset.from_dir(GENERALE_MISSIVEN_DOCUMENT_DIR)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Page(label=<Label.IN: 2>, regions=[Region(id='region_ae5e5a08-694f-4aa0-8626-6b575455ce16_5', types=(<RegionType.TEXT_REGION: 'text_region'>, <RegionType.PAGE_NUMBER: 'page-number'>, <RegionType.PHYSICAL_STRUCTURE_DOC: 'physical_structure_doc'>, <RegionType.PAGEXML_DOC: 'pagexml_doc'>), coordinates=((5085, 244), (5081, 240), (5081, 236), (5068, 224), (5064, 224), (5060, 219), (5047, 219), (5043, 215), (5039, 215), (5035, 219), (5006, 219), (5002, 224), (4989, 224), (4985, 228), (4973, 228), (4969, 232), (4956, 232), (4952, 236), (4940, 236), (4936, 240), (4919, 240), (4915, 244), (4898, 244), (4894, 248), (4878, 248), (4873, 253), (4861, 253), (4857, 257), (4853, 257), (4840, 269), (4840, 286), (4836, 290), (4836, 302), (4832, 306), (4832, 344), (4849, 360), (4857, 360), (4861, 364), (4898, 364), (4902, 360), (4911, 360), (4915, 356), (5035, 356), (5039, 352), (5043, 352), (5047, 348), (5056, 348), (5068, 335), (5068, 327), (5072, 323), (5072, 306), (5076, 302), (5076, 286), (5081, 281), (5081, 277), (5085, 273), (5085, 261), (5089, 257), (5085, 253)), lines=('2500',)), Region(id='region_ae02e415-afa1-48f8-aeef-2ad2960df7fc_8', types=(<RegionType.TEXT_REGION: 'text_region'>, <RegionType.MARGINALIA: 'marginalia'>, <RegionType.PHYSICAL_STRUCTURE_DOC: 'physical_structure_doc'>, <RegionType.PAGEXML_DOC: 'pagexml_doc'>), coordinates=((451, 882), (455, 886), (455, 907), (460, 911), (460, 915), (468, 923), (468, 927), (472, 927), (480, 936), (501, 936), (505, 940), (551, 940), (555, 936), (580, 936), (584, 931), (613, 931), (617, 927), (675, 927), (679, 931), (725, 931), (729, 936), (870, 936), (874, 931), (886, 931), (894, 923), (899, 923), (907, 915), (911, 915), (915, 911), (923, 911), (927, 907), (940, 907), (944, 902), (948, 902), (956, 894), (961, 894), (965, 890), (977, 890), (981, 886), (998, 886), (1002, 882), (1027, 882), (1031, 878), (1048, 878), (1056, 869), (1060, 869), (1060, 861), (1064, 857), (1064, 844), (1068, 840), (1068, 828), (1072, 824), (1072, 820), (1077, 816), (1077, 807), (1081, 803), (1081, 766), (1077, 762), (1077, 753), (1068, 745), (1064, 745), (1060, 741), (1056, 741), (1052, 737), (1043, 737), (1039, 733), (1027, 733), (1023, 729), (1006, 729), (1002, 724), (969, 724), (965, 720), (961, 720), (956, 716), (952, 716), (948, 712), (787, 712), (783, 708), (683, 708), (679, 704), (650, 704), (646, 700), (613, 700), (609, 695), (518, 695), (513, 700), (509, 700), (505, 704), (501, 704), (497, 708), (493, 708), (480, 720), (480, 724), (472, 733), (472, 741), (468, 745), (468, 749), (464, 753), (464, 762), (460, 766), (460, 807), (455, 811), (455, 840), (451, 844)), lines=(\"die voor 's Comp:s pant„\", 'verdeet zyn')), Region(id='region_75d25d3b-558b-476b-b1d6-459054620de2_7', types=(<RegionType.TEXT_REGION: 'text_region'>, <RegionType.MARGINALIA: 'marginalia'>, <RegionType.PHYSICAL_STRUCTURE_DOC: 'physical_structure_doc'>, <RegionType.PAGEXML_DOC: 'pagexml_doc'>), coordinates=((435, 2438), (435, 2480), (431, 2484), (431, 2496), (435, 2500), (435, 2517), (439, 2521), (439, 2600), (443, 2604), (443, 2612), (451, 2620), (451, 2625), (455, 2625), (460, 2629), (464, 2629), (468, 2633), (476, 2633), (480, 2637), (489, 2637), (493, 2641), (497, 2641), (501, 2645), (513, 2645), (518, 2649), (547, 2649), (551, 2645), (571, 2645), (576, 2641), (605, 2641), (609, 2637), (638, 2637), (642, 2633), (741, 2633), (745, 2629), (762, 2629), (766, 2625), (803, 2625), (807, 2620), (812, 2620), (816, 2616), (828, 2616), (832, 2612), (841, 2612), (845, 2608), (861, 2608), (865, 2604), (874, 2604), (878, 2600), (890, 2600), (894, 2596), (907, 2596), (911, 2591), (927, 2591), (932, 2596), (936, 2596), (940, 2591), (952, 2591), (956, 2587), (965, 2587), (969, 2583), (1019, 2583), (1023, 2579), (1039, 2579), (1043, 2575), (1048, 2575), (1056, 2567), (1060, 2567), (1068, 2558), (1077, 2558), (1081, 2554), (1081, 2546), (1085, 2542), (1085, 2525), (1089, 2521), (1089, 2496), (1085, 2492), (1085, 2488), (1081, 2484), (1081, 2480), (1077, 2476), (1077, 2430), (1072, 2426), (1072, 2422), (1056, 2405), (1014, 2405), (1010, 2401), (961, 2401), (956, 2397), (936, 2397), (932, 2393), (899, 2393), (894, 2389), (849, 2389), (845, 2393), (836, 2393), (832, 2389), (762, 2389), (758, 2384), (497, 2384), (493, 2389), (489, 2389), (484, 2393), (480, 2393), (476, 2397), (472, 2397), (468, 2401), (464, 2401), (460, 2405), (455, 2405), (451, 2409), (451, 2413), (443, 2422), (443, 2426), (439, 2430), (439, 2434)), lines=('omwelke reden 3.', 'pantchiallangs vrogt', 'werden')), Region(id='region_2377d1f8-5576-4090-867e-f0faa2aa830a_6', types=(<RegionType.TEXT_REGION: 'text_region'>, <RegionType.MARGINALIA: 'marginalia'>, <RegionType.PHYSICAL_STRUCTURE_DOC: 'physical_structure_doc'>, <RegionType.PAGEXML_DOC: 'pagexml_doc'>), coordinates=((2770, 3328), (2770, 3415), (2774, 3419), (2774, 3440), (2778, 3444), (2778, 3448), (2782, 3452), (2787, 3452), (2799, 3465), (2803, 3465), (2807, 3469), (2828, 3469), (2832, 3473), (2869, 3473), (2874, 3477), (2944, 3477), (2948, 3473), (2994, 3473), (2998, 3469), (3018, 3469), (3023, 3465), (3068, 3465), (3072, 3461), (3089, 3461), (3093, 3457), (3110, 3457), (3114, 3452), (3126, 3452), (3130, 3448), (3139, 3448), (3147, 3440), (3151, 3440), (3155, 3436), (3163, 3436), (3168, 3432), (3192, 3432), (3197, 3428), (3205, 3428), (3209, 3423), (3213, 3423), (3217, 3419), (3234, 3419), (3238, 3415), (3267, 3415), (3271, 3411), (3296, 3411), (3300, 3407), (3312, 3407), (3317, 3403), (3333, 3403), (3337, 3399), (3346, 3399), (3350, 3395), (3358, 3395), (3362, 3390), (3370, 3390), (3370, 3328), (3366, 3324), (3362, 3324), (3354, 3316), (3350, 3316), (3333, 3299), (3325, 3299), (3321, 3295), (3317, 3295), (3308, 3287), (3300, 3287), (3296, 3283), (3283, 3283), (3279, 3279), (3271, 3279), (3267, 3274), (3255, 3274), (3250, 3270), (3230, 3270), (3226, 3266), (3209, 3266), (3205, 3262), (3192, 3262), (3188, 3258), (3172, 3258), (3168, 3254), (3155, 3254), (3151, 3250), (2969, 3250), (2965, 3254), (2936, 3254), (2932, 3258), (2919, 3258), (2915, 3262), (2898, 3262), (2894, 3266), (2882, 3266), (2878, 3270), (2861, 3270), (2857, 3274), (2840, 3274), (2836, 3279), (2824, 3279), (2820, 3283), (2816, 3283), (2811, 3287), (2807, 3287), (2778, 3316), (2778, 3320)), lines=('maer de Z: O:t Eijlanden', 'vertrocken')), Region(id='region_215df0f8-00e7-4681-8390-54192d275218_1', types=(<RegionType.TEXT_REGION: 'text_region'>, <RegionType.PHYSICAL_STRUCTURE_DOC: 'physical_structure_doc'>, <RegionType.PARAGRAPH: 'paragraph'>, <RegionType.PAGEXML_DOC: 'pagexml_doc'>), coordinates=((2774, 592), (2720, 588), (2592, 609), (2261, 604), (1979, 617), (1793, 617), (1648, 604), (1246, 613), (1192, 604), (1126, 638), (1118, 687), (1122, 700), (1114, 716), (1077, 749), (1085, 766), (1085, 803), (1064, 861), (1085, 915), (1081, 956), (1101, 1076), (1106, 1151), (1139, 1279), (1135, 1441), (1143, 1478), (1143, 1697), (1130, 1784), (1135, 1991), (1122, 2120), (1126, 2277), (1093, 2405), (1101, 2480), (1093, 2492), (1085, 2546), (1097, 2600), (1093, 2687), (1110, 2803), (1126, 3154), (1139, 3217), (1135, 3403), (1101, 3560), (1106, 3589), (1118, 3606), (1164, 3622), (1213, 3622), (1263, 3610), (1337, 3606), (1495, 3614), (2352, 3614), (2414, 3606), (2588, 3601), (2642, 3614), (2708, 3643), (2737, 3639), (2766, 3614), (2782, 3573), (2774, 3444), (2766, 3415), (2766, 3266), (2778, 3159), (2762, 3039), (2766, 2774), (2749, 2654), (2753, 2492), (2745, 2447), (2741, 2273), (2758, 2198), (2766, 1962), (2758, 1896), (2753, 1143), (2762, 1068), (2758, 907), (2799, 695), (2795, 625)), lines=('Pantchiallang niet bang waren', '„chiallang niet de„ maar deselve kunnende magt', 'werden wel wat anders zoud', 'doen tot wraak over het nem', 'van Een hunner vaartuijgen', \"onder 't Eijland Wokam in den\", \"voorleden Iare 't geen de minist\", 'heeft gepermoveert om bij voorn', 'briev van den 15„en aug:s in plaats', 'van de pantchiallangs Nova', 'hollandia, Lethij, en de voorn,', 'alsoo de eerste in den voorleden', 'Jaare afgelopen en verbrand', 'de tweede vermist is, en de derd', 'werden om 3. nieuwe niet')), Region(id='region_963c3538-4b10-4988-aaa6-46b65c8d3440_2', types=(<RegionType.TEXT_REGION: 'text_region'>, <RegionType.PHYSICAL_STRUCTURE_DOC: 'physical_structure_doc'>, <RegionType.PARAGRAPH: 'paragraph'>, <RegionType.PAGEXML_DOC: 'pagexml_doc'>), coordinates=((5064, 567), (5023, 563), (4882, 604), (4782, 617), (4563, 596), (3731, 604), (3470, 617), (3424, 629), (3399, 650), (3399, 758), (3387, 861), (3395, 882), (3395, 1051), (3416, 1283), (3416, 1706), (3404, 1888), (3412, 2774), (3391, 2935), (3391, 3179), (3412, 3233), (3412, 3274), (3375, 3328), (3375, 3395), (3408, 3440), (3391, 3515), (3391, 3577), (3428, 3614), (3495, 3626), (3602, 3610), (4811, 3601), (5085, 3618), (5118, 3597), (5130, 3552), (5122, 3254), (5101, 3167), (5089, 2972), (5056, 2931), (4890, 2939), (4816, 2923), (4646, 2914), (4045, 2931), (3710, 2902), (3611, 2914), (3569, 2889), (3573, 2869), (3598, 2852), (3822, 2860), (3946, 2819), (3979, 2790), (3971, 2728), (3987, 2691), (4008, 2670), (4066, 2654), (4174, 2645), (4493, 2654), (4571, 2645), (4956, 2649), (5047, 2637), (5081, 2616), (5097, 2567), (5093, 2418), (5101, 2393), (5089, 2074), (5105, 1718), (5105, 1022), (5085, 886), (5089, 609)), lines=('alleen aan ons versoek, maar', 'ook een voorslag tedoen, ten', 'Eijnde in plaatse van een, drie', 'wel bemande pantchiallangs', 'op die swervers uijt tesenden,', 'en des mogelijk te overmeesteren', 'dan wel van daar te verjagen', 'waar op men bij het houden der', 'besoignes de nodige reflexie', 'zal nemen en den Eijsch dier', 'vaartuijgen naar vermogen', 'voldoen.', \"Voorts was op den 17:' maart\", 'deses Iaars om de posthouders', 'nodige naar usantie te voorsten', 'door ouderdom staat afgelegt te zijn gecommitteerd:s op de Z: O„ter Eijlanden van het')), Region(id='region_40149bda-3718-474c-973a-f35555897339_3', types=(<RegionType.TEXT_REGION: 'text_region'>, <RegionType.CATCH_WORD: 'catch-word'>, <RegionType.PHYSICAL_STRUCTURE_DOC: 'physical_structure_doc'>, <RegionType.PAGEXML_DOC: 'pagexml_doc'>), coordinates=((2331, 3717), (2331, 3759), (2335, 3763), (2335, 3767), (2339, 3771), (2339, 3775), (2356, 3792), (2364, 3792), (2368, 3796), (2381, 3796), (2385, 3800), (2414, 3800), (2418, 3796), (2501, 3796), (2505, 3800), (2563, 3800), (2567, 3804), (2575, 3804), (2580, 3800), (2613, 3800), (2617, 3796), (2638, 3796), (2642, 3792), (2646, 3792), (2658, 3779), (2662, 3779), (2679, 3763), (2679, 3759), (2683, 3755), (2683, 3730), (2679, 3726), (2679, 3722), (2662, 3705), (2658, 3705), (2650, 3697), (2646, 3697), (2642, 3693), (2443, 3693), (2439, 3688), (2414, 3688), (2410, 3684), (2385, 3684), (2381, 3680), (2364, 3680), (2360, 3684), (2356, 3684), (2348, 3693), (2348, 3697), (2335, 3709), (2335, 3713)), lines=('alleen',)), Region(id='region_a86603f9-3214-47cd-8419-070ca5a71481_4', types=(<RegionType.TEXT_REGION: 'text_region'>, <RegionType.CATCH_WORD: 'catch-word'>, <RegionType.PHYSICAL_STRUCTURE_DOC: 'physical_structure_doc'>, <RegionType.PAGEXML_DOC: 'pagexml_doc'>), coordinates=((4604, 3717), (4604, 3763), (4608, 3767), (4608, 3771), (4613, 3775), (4613, 3779), (4625, 3792), (4629, 3792), (4633, 3796), (4637, 3796), (4642, 3800), (4654, 3800), (4658, 3804), (4691, 3804), (4695, 3808), (4766, 3808), (4770, 3813), (4820, 3813), (4824, 3808), (4840, 3808), (4844, 3804), (4853, 3804), (4857, 3800), (4873, 3800), (4878, 3796), (4882, 3796), (4894, 3784), (4894, 3779), (4898, 3775), (4898, 3767), (4902, 3763), (4902, 3742), (4898, 3738), (4898, 3730), (4894, 3726), (4894, 3722), (4890, 3717), (4890, 3713), (4886, 3709), (4886, 3705), (4878, 3697), (4869, 3697), (4865, 3693), (4857, 3693), (4853, 3688), (4840, 3688), (4836, 3684), (4791, 3684), (4787, 3680), (4774, 3680), (4770, 3676), (4749, 3676), (4745, 3680), (4733, 3680), (4729, 3684), (4708, 3684), (4704, 3680), (4654, 3680), (4650, 3684), (4637, 3684), (4633, 3688), (4625, 3688), (4613, 3701), (4613, 3705), (4608, 3709), (4608, 3713)), lines=('met',)), Region(id='region_394c918a-b71a-400a-aa3b-ed6b59cc96c7', types=(<RegionType.TEXT_REGION: 'text_region'>, <RegionType.PHYSICAL_STRUCTURE_DOC: 'physical_structure_doc'>, <RegionType.PAGEXML_DOC: 'pagexml_doc'>), coordinates=((4352, 2787), (4445, 2787), (4445, 2806), (4352, 2806)), lines=('0',))], scan_nr=57, doc_id='NL-HaNA_1.04.02_2113_0057.jpg')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = dataset[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset[10000:11000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carstens/workspace/.venv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "%autoreload now\n",
    "\n",
    "from document_segmentation.model.page_sequence_tagger import PageSequenceTagger\n",
    "\n",
    "tagger = PageSequenceTagger(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger._device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PageSequenceTagger(\n",
       "  (_page_embedding): PageEmbedding(\n",
       "    (_region_model): RegionEmbedding(\n",
       "      (_transformer_model): BertModel(\n",
       "        (embeddings): BertEmbeddings(\n",
       "          (word_embeddings): Embedding(30500, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (token_type_embeddings): Embedding(2, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): BertEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (pooler): BertPooler(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "      )\n",
       "      (_region_embedding): Embedding(9, 16)\n",
       "      (_linear): Linear(in_features=784, out_features=128, bias=True)\n",
       "    )\n",
       "    (_rnn): LSTM(128, 128, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "    (_linear): Linear(in_features=256, out_features=64, bias=True)\n",
       "  )\n",
       "  (_rnn): LSTM(64, 128, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "  (_linear): Linear(in_features=256, out_features=3, bias=True)\n",
       "  (_softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 36/1250.0 [02:08<1:16:44,  3.79s/batch]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  3%|▎         | 36/1250.0 [02:09<1:12:50,  3.60s/batch]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 684.00 MiB. GPU 0 has a total capacity of 5.81 GiB of which 548.06 MiB is free. Including non-PyTorch memory, this process has 5.26 GiB memory in use. Of the allocated memory 4.52 GiB is allocated by PyTorch, and 626.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautonotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/document_segmentation/document_segmentation/model/page_sequence_tagger.py:91\u001b[0m, in \u001b[0;36mPageSequenceTagger.train_\u001b[0;34m(self, pages, epochs, batch_size, weights)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m     88\u001b[0m     pages\u001b[38;5;241m.\u001b[39mbatches(batch_size), unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(pages) \u001b[38;5;241m/\u001b[39m batch_size\n\u001b[1;32m     89\u001b[0m ):\n\u001b[1;32m     90\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 91\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device)\n\u001b[1;32m     92\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch\u001b[38;5;241m.\u001b[39mlabel_tensor()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device))\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device\n\u001b[1;32m     94\u001b[0m     )\n\u001b[1;32m     96\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/document_segmentation/document_segmentation/model/page_sequence_tagger.py:52\u001b[0m, in \u001b[0;36mPageSequenceTagger.forward\u001b[0;34m(self, pages)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pages: PageDataset):\n\u001b[0;32m---> 52\u001b[0m     page_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_page_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m page_embeddings\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m (\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28mlen\u001b[39m(pages),\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_page_embedding\u001b[38;5;241m.\u001b[39moutput_size,\n\u001b[1;32m     57\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBad shape: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mpages.size()}\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m     rnn_out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rnn(page_embeddings)\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/document_segmentation/document_segmentation/model/page_embedding.py:71\u001b[0m, in \u001b[0;36mPageEmbedding.forward\u001b[0;34m(self, pages)\u001b[0m\n\u001b[1;32m     62\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToo many regions (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(regions_batch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), truncating to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_regions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m     region_inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     66\u001b[0m         regions_batch[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_regions \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;241m+\u001b[39m regions_batch[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_regions \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m :]\n\u001b[1;32m     68\u001b[0m     )\n\u001b[1;32m     70\u001b[0m region_inputs \u001b[38;5;241m=\u001b[39m pad_sequence(\n\u001b[0;32m---> 71\u001b[0m     [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_region_model(regions) \u001b[38;5;28;01mfor\u001b[39;00m regions \u001b[38;5;129;01min\u001b[39;00m regions_batch],\n\u001b[1;32m     72\u001b[0m     batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     73\u001b[0m     padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     74\u001b[0m )\n\u001b[1;32m     76\u001b[0m rnn_out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rnn(region_inputs)\n\u001b[1;32m     78\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_linear(rnn_out)\n",
      "File \u001b[0;32m~/workspace/document_segmentation/document_segmentation/model/page_embedding.py:71\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     62\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToo many regions (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(regions_batch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), truncating to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_regions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m     region_inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     66\u001b[0m         regions_batch[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_regions \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;241m+\u001b[39m regions_batch[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_regions \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m :]\n\u001b[1;32m     68\u001b[0m     )\n\u001b[1;32m     70\u001b[0m region_inputs \u001b[38;5;241m=\u001b[39m pad_sequence(\n\u001b[0;32m---> 71\u001b[0m     [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_region_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregions\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m regions \u001b[38;5;129;01min\u001b[39;00m regions_batch],\n\u001b[1;32m     72\u001b[0m     batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     73\u001b[0m     padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     74\u001b[0m )\n\u001b[1;32m     76\u001b[0m rnn_out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rnn(region_inputs)\n\u001b[1;32m     78\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_linear(rnn_out)\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/document_segmentation/document_segmentation/model/region_embedding.py:151\u001b[0m, in \u001b[0;36mRegionEmbedding.forward\u001b[0;34m(self, regions)\u001b[0m\n\u001b[1;32m    146\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting regions into a tuple (was: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(regions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m     )\n\u001b[1;32m    149\u001b[0m     regions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(regions)\n\u001b[0;32m--> 151\u001b[0m text_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mregions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    152\u001b[0m region_types \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_region_embedding(\n\u001b[1;32m    154\u001b[0m         torch\u001b[38;5;241m.\u001b[39mIntTensor(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_region_embedding\u001b[38;5;241m.\u001b[39membedding_dim)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device)\n\u001b[1;32m    160\u001b[0m )\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# region_coordinates = self._coordinates_tensor(region).float() TODO\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/document_segmentation/document_segmentation/model/region_embedding.py:101\u001b[0m, in \u001b[0;36mRegionEmbedding._text_embeddings\u001b[0;34m(self, region_batch)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m region_texts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(text \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m region_texts):\n\u001b[1;32m     93\u001b[0m     text_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer(\n\u001b[1;32m     94\u001b[0m         region_texts,\n\u001b[1;32m     95\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m         max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_length,\n\u001b[1;32m     99\u001b[0m     )\n\u001b[0;32m--> 101\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformer_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtext_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    104\u001b[0m     cls_tokens \u001b[38;5;241m=\u001b[39m out[:, \u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# CLS token is first token of sequence\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:325\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    322\u001b[0m     past_key_value \u001b[38;5;241m=\u001b[39m (key_layer, value_layer)\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[0;32m--> 325\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key_query\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    328\u001b[0m     query_length, key_length \u001b[38;5;241m=\u001b[39m query_layer\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], key_layer\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 684.00 MiB. GPU 0 has a total capacity of 5.81 GiB of which 548.06 MiB is free. Including non-PyTorch memory, this process has 5.26 GiB memory in use. Of the allocated memory 4.52 GiB is allocated by PyTorch, and 626.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "tagger.train_(training_dataset, epochs=3, weights=dataset.class_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MulticlassPrecision:   0%|          | 0/3.125 [00:00<?, ?batch/s]WARNING:root:tensor([[1]]) classes have zero instances in both the predictions and the ground truth labels. Precision is still logged as zero.\n",
      "MulticlassPrecision:  32%|███▏      | 1/3.125 [00:02<00:05,  2.63s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MulticlassPrecision: {'BEGIN': 0.0, 'IN': 0.0, 'END': 0.0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:tensor([[1]]) classes have zero instances in both the predictions and the ground truth labels. Precision is still logged as zero.\n",
      "MulticlassPrecision:  64%|██████▍   | 2/3.125 [00:05<00:02,  2.50s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MulticlassPrecision: {'BEGIN': 0.0, 'IN': 0.0, 'END': 0.0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:tensor([[1]]) classes have zero instances in both the predictions and the ground truth labels. Precision is still logged as zero.\n",
      "MulticlassPrecision:  96%|█████████▌| 3/3.125 [00:07<00:00,  2.65s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MulticlassPrecision: {'BEGIN': 0.0, 'IN': 0.0, 'END': 0.0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:tensor([[1]]) classes have zero instances in both the predictions and the ground truth labels. Precision is still logged as zero.\n",
      "MulticlassPrecision: 4batch [00:08,  2.05s/batch]                        \n",
      "WARNING:root:tensor([[1]]) classes have zero instances in both the predictions and the ground truth labels. Precision is still logged as zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MulticlassPrecision: {'BEGIN': 0.0, 'IN': 0.0, 'END': 0.0}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BEGIN': 0.0, 'IN': 0.0, 'END': 0.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.precision(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MulticlassRecall:   0%|          | 0/3.125 [00:00<?, ?batch/s]WARNING:root:One or more NaNs identified, as no ground-truth instances of [0, 2] have been seen. These have been converted to zero.\n",
      "MulticlassRecall:   0%|          | 0/3.125 [00:00<?, ?batch/s]WARNING:root:One or more NaNs identified, as no ground-truth instances of [0, 2] have been seen. These have been converted to zero.\n",
      "MulticlassRecall:   0%|          | 0/3.125 [00:00<?, ?batch/s]WARNING:root:One or more NaNs identified, as no ground-truth instances of [0, 2] have been seen. These have been converted to zero.\n",
      "MulticlassRecall:   0%|          | 0/3.125 [00:00<?, ?batch/s]WARNING:root:One or more NaNs identified, as no ground-truth instances of [0, 2] have been seen. These have been converted to zero.\n",
      "MulticlassRecall: 4batch [00:00, 100.55batch/s]               \n",
      "WARNING:root:One or more NaNs identified, as no ground-truth instances of [0, 2] have been seen. These have been converted to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MulticlassRecall: {'BEGIN': 0.0, 'IN': 0.0, 'END': 0.0}]\n",
      "[MulticlassRecall: {'BEGIN': 0.0, 'IN': 0.0, 'END': 0.0}]\n",
      "[MulticlassRecall: {'BEGIN': 0.0, 'IN': 0.0, 'END': 0.0}]\n",
      "[MulticlassRecall: {'BEGIN': 0.0, 'IN': 0.0, 'END': 0.0}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BEGIN': 0.0, 'IN': 0.0, 'END': 0.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.recall(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MulticlassF1Score:   0%|          | 0/3.125 [00:00<?, ?batch/s]WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n",
      "MulticlassF1Score:   0%|          | 0/3.125 [00:00<?, ?batch/s]WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n",
      "MulticlassF1Score:   0%|          | 0/3.125 [00:00<?, ?batch/s]WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n",
      "MulticlassF1Score:   0%|          | 0/3.125 [00:00<?, ?batch/s]WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n",
      "MulticlassF1Score: 4batch [00:00, 39.01batch/s]                \n",
      "WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MulticlassF1Score: {'BEGIN': 0.0, 'IN': 0.0, 'END': 0.0}]\n",
      "[MulticlassF1Score: {'BEGIN': 0.0, 'IN': 0.0, 'END': 0.0}]\n",
      "[MulticlassF1Score: {'BEGIN': 0.0, 'IN': 0.0, 'END': 0.0}]\n",
      "[MulticlassF1Score: {'BEGIN': 0.0, 'IN': 0.0, 'END': 0.0}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BEGIN': 0.0, 'IN': 0.0, 'END': 0.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.f1_score(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MulticlassAccuracy:   0%|          | 0/3.125 [00:00<?, ?batch/s]/home/carstens/workspace/.venv/lib/python3.10/site-packages/torcheval/metrics/functional/classification/accuracy.py:275: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:232.)\n",
      "  num_correct = mask.new_zeros(num_classes).scatter_(0, target, mask, reduce=\"add\")\n",
      "MulticlassAccuracy:   0%|          | 0/3.125 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MulticlassAccuracy: {'BEGIN': nan, 'IN': 0.0, 'END': nan}]\n",
      "[MulticlassAccuracy: {'BEGIN': nan, 'IN': 0.0, 'END': nan}]\n",
      "[MulticlassAccuracy: {'BEGIN': nan, 'IN': 0.0, 'END': nan}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MulticlassAccuracy: 4batch [00:00, 108.92batch/s]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MulticlassAccuracy: {'BEGIN': nan, 'IN': 0.0, 'END': nan}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.accuracy(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page ID\tTrue Label\tPredicted Label\tCorrect?\tPredicted Scores\n",
      "NL-HaNA_1.04.02_3340_0601.jpg\tIN\tIN\tTrue\t[0.002934614662081003, 0.9970417618751526, 2.3641650841454975e-05]\n",
      "NL-HaNA_1.04.02_3340_0602.jpg\tIN\tIN\tTrue\t[9.64675418799743e-05, 0.9999003410339355, 3.2149832804861944e-06]\n",
      "NL-HaNA_1.04.02_3340_0603.jpg\tIN\tIN\tTrue\t[3.116503285127692e-05, 0.9999668598175049, 2.0005293208669173e-06]\n",
      "NL-HaNA_1.04.02_3340_0604.jpg\tIN\tIN\tTrue\t[2.2477948732557707e-05, 0.9999756813049316, 1.7827460396802053e-06]\n",
      "NL-HaNA_1.04.02_3340_0605.jpg\tIN\tIN\tTrue\t[2.038254024228081e-05, 0.9999779462814331, 1.7279730855079833e-06]\n",
      "NL-HaNA_1.04.02_3340_0606.jpg\tIN\tIN\tTrue\t[1.9676292140502483e-05, 0.9999786615371704, 1.709656544335303e-06]\n",
      "NL-HaNA_1.04.02_3340_0607.jpg\tIN\tIN\tTrue\t[1.9373219402041286e-05, 0.9999788999557495, 1.7021116036630701e-06]\n",
      "NL-HaNA_1.04.02_3340_0608.jpg\tIN\tIN\tTrue\t[1.9227041775593534e-05, 0.9999790191650391, 1.6983323121166904e-06]\n",
      "NL-HaNA_1.04.02_3340_0609.jpg\tIN\tIN\tTrue\t[1.914861422847025e-05, 0.9999791383743286, 1.6961764686129754e-06]\n",
      "NL-HaNA_1.04.02_3340_0610.jpg\tIN\tIN\tTrue\t[1.9107716070720926e-05, 0.9999791383743286, 1.6946499954428873e-06]\n",
      "NL-HaNA_1.04.02_3340_0611.jpg\tIN\tIN\tTrue\t[1.9043047359446064e-05, 0.9999792575836182, 1.6950447161434568e-06]\n",
      "NL-HaNA_1.04.02_3340_0612.jpg\tIN\tIN\tTrue\t[1.905514727695845e-05, 0.9999792575836182, 1.6937261761995615e-06]\n",
      "NL-HaNA_1.04.02_3340_0613.jpg\tIN\tIN\tTrue\t[1.9017166778212413e-05, 0.9999792575836182, 1.6941460216912674e-06]\n",
      "NL-HaNA_1.04.02_3340_0614.jpg\tIN\tIN\tTrue\t[1.903614611364901e-05, 0.9999792575836182, 1.693096237431746e-06]\n",
      "NL-HaNA_1.04.02_3340_0615.jpg\tIN\tIN\tTrue\t[1.9020359104615636e-05, 0.9999792575836182, 1.6931544450926594e-06]\n",
      "NL-HaNA_1.04.02_3340_0616.jpg\tIN\tIN\tTrue\t[1.90416303667007e-05, 0.9999792575836182, 1.6926634316405398e-06]\n",
      "NL-HaNA_1.04.02_3340_0617.jpg\tIN\tIN\tTrue\t[1.9018381863133982e-05, 0.9999792575836182, 1.6927668866628665e-06]\n",
      "NL-HaNA_1.04.02_3340_0618.jpg\tIN\tIN\tTrue\t[1.901393807202112e-05, 0.9999792575836182, 1.6926876469369745e-06]\n",
      "NL-HaNA_1.04.02_3340_0619.jpg\tIN\tIN\tTrue\t[1.8936128981295042e-05, 0.9999793767929077, 1.6956236095211352e-06]\n",
      "NL-HaNA_1.04.02_3340_0620.jpg\tIN\tIN\tTrue\t[1.903974225569982e-05, 0.9999792575836182, 1.692979822109919e-06]\n",
      "NL-HaNA_1.04.02_3340_0621.jpg\tIN\tIN\tTrue\t[1.90218288480537e-05, 0.9999792575836182, 1.6932948483372456e-06]\n",
      "NL-HaNA_1.04.02_3340_0622.jpg\tIN\tIN\tTrue\t[1.903077463794034e-05, 0.9999792575836182, 1.6929976709434413e-06]\n",
      "NL-HaNA_1.04.02_3340_0623.jpg\tIN\tIN\tTrue\t[1.90221362572629e-05, 0.9999792575836182, 1.6932302742134198e-06]\n",
      "NL-HaNA_1.04.02_3340_0624.jpg\tIN\tIN\tTrue\t[1.90360387932742e-05, 0.9999792575836182, 1.692779733275529e-06]\n",
      "NL-HaNA_1.04.02_3340_0625.jpg\tIN\tIN\tTrue\t[1.902979420265183e-05, 0.9999792575836182, 1.6928411241678987e-06]\n",
      "NL-HaNA_1.04.02_3340_0626.jpg\tIN\tIN\tTrue\t[1.9029739632969722e-05, 0.9999792575836182, 1.692818500487192e-06]\n",
      "NL-HaNA_1.04.02_3340_0627.jpg\tIN\tIN\tTrue\t[1.9013303244719282e-05, 0.9999792575836182, 1.6933610140767996e-06]\n",
      "NL-HaNA_1.04.02_3340_0628.jpg\tIN\tIN\tTrue\t[1.9035274817724712e-05, 0.9999792575836182, 1.692776436357235e-06]\n",
      "NL-HaNA_1.04.02_3340_0629.jpg\tIN\tIN\tTrue\t[1.9031846022699028e-05, 0.9999792575836182, 1.6927829165069852e-06]\n",
      "NL-HaNA_1.04.02_3340_0630.jpg\tIN\tIN\tTrue\t[1.9029212126042694e-05, 0.9999792575836182, 1.6928282775552361e-06]\n",
      "NL-HaNA_1.04.02_3340_0631.jpg\tIN\tIN\tTrue\t[1.9024857465410605e-05, 0.9999792575836182, 1.6929637922658003e-06]\n",
      "NL-HaNA_1.04.02_3340_0632.jpg\tIN\tIN\tTrue\t[1.9029812392545864e-05, 0.9999792575836182, 1.6928346440181485e-06]\n",
      "NL-HaNA_1.04.02_3340_0633.jpg\tIN\tIN\tTrue\t[1.9027726011699997e-05, 0.9999792575836182, 1.692878186076996e-06]\n",
      "NL-HaNA_1.04.02_3340_0634.jpg\tIN\tIN\tTrue\t[1.903073825815227e-05, 0.9999792575836182, 1.692747446213616e-06]\n",
      "NL-HaNA_1.04.02_3340_0635.jpg\tIN\tIN\tTrue\t[1.9034005163121037e-05, 0.9999792575836182, 1.6925247336985194e-06]\n",
      "NL-HaNA_1.04.02_3340_0636.jpg\tIN\tIN\tTrue\t[1.9029195755138062e-05, 0.9999792575836182, 1.69253269177716e-06]\n",
      "NL-HaNA_1.04.02_3340_0637.jpg\tIN\tIN\tTrue\t[1.9021301341126673e-05, 0.9999792575836182, 1.692647401796421e-06]\n",
      "NL-HaNA_1.04.02_3340_0638.jpg\tIN\tIN\tTrue\t[1.9012197299161926e-05, 0.9999792575836182, 1.6929637922658003e-06]\n",
      "NL-HaNA_1.04.02_3340_0639.jpg\tIN\tIN\tTrue\t[1.8993892808794044e-05, 0.9999792575836182, 1.6937649434112245e-06]\n",
      "NL-HaNA_1.04.02_3340_0640.jpg\tIN\tIN\tTrue\t[1.902908661577385e-05, 0.9999792575836182, 1.6928959212236805e-06]\n",
      "NL-HaNA_1.04.02_3340_0641.jpg\tIN\tIN\tTrue\t[1.902257281471975e-05, 0.9999792575836182, 1.6930719084484736e-06]\n",
      "NL-HaNA_1.04.02_3340_0642.jpg\tIN\tIN\tTrue\t[1.9031227566301823e-05, 0.9999792575836182, 1.6928588593145832e-06]\n",
      "NL-HaNA_1.04.02_3340_0643.jpg\tIN\tIN\tTrue\t[1.9027616872335784e-05, 0.9999792575836182, 1.6929056982917245e-06]\n",
      "NL-HaNA_1.04.02_3340_0644.jpg\tIN\tIN\tTrue\t[1.905391036416404e-05, 0.9999792575836182, 1.6918501160034793e-06]\n",
      "NL-HaNA_1.04.02_3340_0645.jpg\tIN\tIN\tTrue\t[1.9011908079846762e-05, 0.9999792575836182, 1.6929734556470066e-06]\n",
      "NL-HaNA_1.04.02_3340_0646.jpg\tIN\tIN\tTrue\t[1.902144867926836e-05, 0.9999792575836182, 1.6927426713664318e-06]\n",
      "NL-HaNA_1.04.02_3340_0647.jpg\tIN\tIN\tTrue\t[1.9006758520845324e-05, 0.9999792575836182, 1.6932254993662355e-06]\n",
      "NL-HaNA_1.04.02_3340_0648.jpg\tIN\tIN\tTrue\t[1.903155498439446e-05, 0.9999792575836182, 1.692444016043737e-06]\n",
      "NL-HaNA_1.04.02_3340_0649.jpg\tIN\tIN\tTrue\t[1.9014409190276638e-05, 0.9999792575836182, 1.6927490378293442e-06]\n",
      "NL-HaNA_1.04.02_3340_0650.jpg\tIN\tIN\tTrue\t[1.9013377823284827e-05, 0.9999792575836182, 1.6928055401876918e-06]\n",
      "NL-HaNA_1.04.02_3340_0651.jpg\tIN\tIN\tTrue\t[1.8964141418109648e-05, 0.9999793767929077, 1.6949350083450554e-06]\n",
      "NL-HaNA_1.04.02_3340_0652.jpg\tIN\tIN\tTrue\t[1.9034876459045336e-05, 0.9999792575836182, 1.6931188611124526e-06]\n",
      "NL-HaNA_1.04.02_3340_0653.jpg\tIN\tIN\tTrue\t[1.903037446027156e-05, 0.9999792575836182, 1.6931704749367782e-06]\n",
      "NL-HaNA_1.04.02_3340_0654.jpg\tIN\tIN\tTrue\t[1.9052384232054465e-05, 0.9999792575836182, 1.6923535213209107e-06]\n",
      "NL-HaNA_1.04.02_3340_0655.jpg\tIN\tIN\tTrue\t[1.9020031686522998e-05, 0.9999792575836182, 1.6932609696596046e-06]\n",
      "NL-HaNA_1.04.02_3340_0656.jpg\tIN\tIN\tTrue\t[1.9030991097679362e-05, 0.9999792575836182, 1.6930219999267138e-06]\n",
      "NL-HaNA_1.04.02_3340_0657.jpg\tIN\tIN\tTrue\t[1.9030483599635772e-05, 0.9999792575836182, 1.6930445099205826e-06]\n",
      "NL-HaNA_1.04.02_3340_0658.jpg\tIN\tIN\tTrue\t[1.9027505913982168e-05, 0.9999792575836182, 1.6932027619986911e-06]\n",
      "NL-HaNA_1.04.02_3340_0659.jpg\tIN\tIN\tTrue\t[1.902148505905643e-05, 0.9999792575836182, 1.6935192661549081e-06]\n",
      "NL-HaNA_1.04.02_3340_0660.jpg\tIN\tIN\tTrue\t[1.9029141185455956e-05, 0.9999792575836182, 1.6934160385062569e-06]\n",
      "NL-HaNA_1.04.02_3340_0661.jpg\tIN\tIN\tTrue\t[1.9027689631911926e-05, 0.9999792575836182, 1.693554850135115e-06]\n",
      "NL-HaNA_1.04.02_3340_0662.jpg\tIN\tIN\tTrue\t[1.9023986169486307e-05, 0.9999792575836182, 1.693879539743648e-06]\n",
      "NL-HaNA_1.04.02_3340_0663.jpg\tIN\tIN\tTrue\t[1.8998351151822135e-05, 0.9999792575836182, 1.6951821635302622e-06]\n",
      "NL-HaNA_1.04.02_3340_0664.jpg\tIN\tIN\tTrue\t[1.903694646898657e-05, 0.9999792575836182, 1.6946568166531506e-06]\n",
      "NL-HaNA_1.04.02_3340_0665.jpg\tIN\tIN\tTrue\t[1.9032644559047185e-05, 0.9999792575836182, 1.6951821635302622e-06]\n",
      "NL-HaNA_1.04.02_3340_0666.jpg\tIN\tIN\tTrue\t[1.9033024727832526e-05, 0.9999792575836182, 1.6957982325038756e-06]\n",
      "NL-HaNA_1.04.02_3340_0667.jpg\tIN\tIN\tTrue\t[1.9016368241864257e-05, 0.9999792575836182, 1.6973190213320777e-06]\n",
      "NL-HaNA_1.04.02_3340_0668.jpg\tIN\tIN\tTrue\t[1.9021701518795453e-05, 0.9999792575836182, 1.6986986111078295e-06]\n",
      "NL-HaNA_1.04.02_3340_0669.jpg\tIN\tIN\tTrue\t[1.8982740584760904e-05, 0.9999792575836182, 1.7022794054355472e-06]\n",
      "NL-HaNA_1.04.02_3340_0670.jpg\tIN\tIN\tTrue\t[1.9004241039510816e-05, 0.9999792575836182, 1.70544979027909e-06]\n",
      "NL-HaNA_1.04.02_3340_0671.jpg\tIN\tIN\tTrue\t[1.8987917428603396e-05, 0.9999792575836182, 1.7120772781709093e-06]\n",
      "NL-HaNA_1.04.02_3340_0672.jpg\tIN\tIN\tTrue\t[1.8929629732156172e-05, 0.9999793767929077, 1.7257359559152974e-06]\n",
      "NL-HaNA_1.04.02_3340_0673.jpg\tIN\tIN\tTrue\t[1.8813079805113375e-05, 0.9999793767929077, 1.754849222379562e-06]\n",
      "NL-HaNA_1.04.02_3340_0674.jpg\tIN\tIN\tTrue\t[1.8725213521975093e-05, 0.9999794960021973, 1.8128694136976264e-06]\n",
      "NL-HaNA_1.04.02_3340_0675.jpg\tIN\tIN\tTrue\t[1.9259574401075952e-05, 0.99997878074646, 1.9032426052945084e-06]\n",
      "NL-HaNA_1.04.02_3340_0676.jpg\tIN\tIN\tTrue\t[2.347843474126421e-05, 0.9999746084213257, 1.9502497252688045e-06]\n",
      "NL-HaNA_1.04.02_3340_0677.jpg\tIN\tIN\tTrue\t[5.9497957408893853e-05, 0.9999388456344604, 1.6234462236752734e-06]\n",
      "NL-HaNA_1.04.02_3340_0678.jpg\tIN\tIN\tTrue\t[1.853523644967936e-05, 0.9999798536300659, 1.6467690784338629e-06]\n",
      "NL-HaNA_1.04.02_3340_0679.jpg\tIN\tIN\tTrue\t[1.8264139725943096e-05, 0.999980092048645, 1.6689324411345297e-06]\n",
      "NL-HaNA_1.04.02_3340_0680.jpg\tIN\tIN\tTrue\t[1.7918846424436197e-05, 0.9999804496765137, 1.6758244782977272e-06]\n",
      "NL-HaNA_1.04.02_3340_0681.jpg\tIN\tIN\tTrue\t[1.7408841813448817e-05, 0.9999809265136719, 1.6822799580040737e-06]\n",
      "NL-HaNA_1.04.02_3340_0682.jpg\tIN\tIN\tTrue\t[1.6689071344444528e-05, 0.9999816417694092, 1.6932521020862623e-06]\n",
      "NL-HaNA_1.04.02_3340_0683.jpg\tIN\tIN\tTrue\t[1.586895450600423e-05, 0.999982476234436, 1.7055543821697938e-06]\n",
      "NL-HaNA_1.04.02_3340_0684.jpg\tIN\tIN\tTrue\t[1.4802808436797932e-05, 0.9999834299087524, 1.7294497638431494e-06]\n",
      "NL-HaNA_1.04.02_3340_0685.jpg\tIN\tIN\tTrue\t[1.3672775821760297e-05, 0.999984622001648, 1.7618140191189013e-06]\n",
      "NL-HaNA_1.04.02_3340_0686.jpg\tIN\tIN\tTrue\t[1.2566542864078656e-05, 0.9999855756759644, 1.8024474002231727e-06]\n",
      "NL-HaNA_1.04.02_3340_0687.jpg\tIN\tIN\tTrue\t[1.1547721442184411e-05, 0.9999866485595703, 1.8509264236854506e-06]\n",
      "NL-HaNA_1.04.02_3340_0688.jpg\tIN\tIN\tTrue\t[1.0663875400496181e-05, 0.9999874830245972, 1.9049751927013858e-06]\n",
      "NL-HaNA_1.04.02_3340_0689.jpg\tIN\tIN\tTrue\t[9.872804184851702e-06, 0.9999881982803345, 1.969332743101404e-06]\n",
      "NL-HaNA_1.04.02_3340_0690.jpg\tIN\tIN\tTrue\t[9.135019354289398e-06, 0.9999887943267822, 2.0500381197052775e-06]\n",
      "NL-HaNA_1.04.02_3340_0691.jpg\tIN\tIN\tTrue\t[8.400135811825749e-06, 0.99998939037323, 2.1576336166617693e-06]\n",
      "NL-HaNA_1.04.02_3340_0692.jpg\tIN\tIN\tTrue\t[7.618183644808596e-06, 0.9999901056289673, 2.3099178179109003e-06]\n",
      "NL-HaNA_1.04.02_3340_0693.jpg\tIN\tIN\tTrue\t[6.728081643814221e-06, 0.999990701675415, 2.542632500990294e-06]\n",
      "NL-HaNA_1.04.02_3340_0694.jpg\tIN\tIN\tTrue\t[5.712324309570249e-06, 0.9999914169311523, 2.9157617973396555e-06]\n",
      "NL-HaNA_1.04.02_3340_0695.jpg\tIN\tIN\tTrue\t[4.5832998694095295e-06, 0.9999918937683105, 3.554083377821371e-06]\n",
      "NL-HaNA_1.04.02_3340_0696.jpg\tIN\tIN\tTrue\t[3.4331324059166946e-06, 0.9999918937683105, 4.7176367843349e-06]\n",
      "NL-HaNA_1.04.02_3340_0697.jpg\tIN\tIN\tTrue\t[2.3708457774773706e-06, 0.9999904632568359, 7.147688393160934e-06]\n",
      "NL-HaNA_1.04.02_3340_0698.jpg\tIN\tIN\tTrue\t[1.542182531011349e-06, 0.9999855756759644, 1.2860488823207561e-05]\n",
      "NL-HaNA_1.04.02_3340_0699.jpg\tIN\tIN\tTrue\t[9.891887202684302e-07, 0.9999704360961914, 2.856998617062345e-05]\n",
      "NL-HaNA_1.04.02_3340_0700.jpg\tIN\tIN\tTrue\t[6.892410056025255e-07, 0.9999223947525024, 7.690579514019191e-05]\n"
     ]
    }
   ],
   "source": [
    "from document_segmentation.pagexml.datamodel.page import Label\n",
    "\n",
    "\n",
    "preds = tagger(test_dataset)\n",
    "\n",
    "print(\n",
    "    \"\\t\".join(\n",
    "        (\"Page ID\", \"True Label\", \"Predicted Label\", \"Correct?\", \"Predicted Scores\")\n",
    "    )\n",
    ")\n",
    "for page_id, true_label, pred, pred_label in zip(\n",
    "    test_dataset.doc_ids(),\n",
    "    test_dataset.labels(),\n",
    "    preds,\n",
    "    preds.argmax(dim=1),\n",
    "    strict=True,\n",
    "):\n",
    "    print(\n",
    "        \"\\t\".join(\n",
    "            (\n",
    "                str(page_id),\n",
    "                true_label.name,\n",
    "                Label(pred_label.item() + 1).name,\n",
    "                str(Label(pred_label.item() + 1) == true_label),\n",
    "                str(pred.tolist()),\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchview'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchview\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m draw_graph\n\u001b[1;32m      3\u001b[0m model_graph \u001b[38;5;241m=\u001b[39m draw_graph(tagger)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_graph\u001b[38;5;241m.\u001b[39mvisual_graph)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchview'"
     ]
    }
   ],
   "source": [
    "from torchview import draw_graph\n",
    "\n",
    "model_graph = draw_graph(tagger)\n",
    "print(model_graph.visual_graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
